{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Detection: Building the Classifier: #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This Jupyter notebook finds the optimal feature set, performs grid search for classifier parameters and then runs the classifer on scaled dataset.**\n",
    "\n",
    "**The best classifier and scaler object are saved for use in detection (see 'Vehicle Detection' notebook). **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot\n",
    "from skimage.feature import hog\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_dir = 'image_dataset/vehicles/'\n",
    "non_vehicle_dir = 'image_dataset/non-vehicles/'\n",
    "\n",
    "car_images = glob.glob(vehicle_dir + '**/*.png', recursive=True)\n",
    "notcar_images = glob.glob(non_vehicle_dir + '**/*.png', recursive=True)\n",
    "\n",
    "cars = []\n",
    "notcars = []\n",
    "\n",
    "for img in car_images:\n",
    "    cars.append(img)\n",
    "for img in notcar_images:\n",
    "    notcars.append(img)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Taken from Udacity SDC lessons\n",
    "def convert_color(img, conv='RGB2YCrCb'):\n",
    "    if conv == 'RGB2YCrCb':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    if conv == 'BGR2YCrCb':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
    "    if conv == 'RGB2LUV':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "    if conv == 'BGR2LUV':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2LUV)\n",
    "    if conv == 'BGR2HLS':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2HLS)\n",
    "    if conv == 'BGR2HSV':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, \n",
    "                        vis=False, feature_vec=True):\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, \n",
    "                                  pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), \n",
    "                                  transform_sqrt=True, \n",
    "                                  visualise=vis, feature_vector=feature_vec, block_norm='L2')\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, \n",
    "                       pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), \n",
    "                       transform_sqrt=True, \n",
    "                       visualise=vis, feature_vector=feature_vec, block_norm='L2')\n",
    "        return features\n",
    "\n",
    "def bin_spatial(img, size=(32, 32)):\n",
    "    color1 = cv2.resize(img[:,:,0], size).ravel()\n",
    "    color2 = cv2.resize(img[:,:,1], size).ravel()\n",
    "    color3 = cv2.resize(img[:,:,2], size).ravel()\n",
    "    return np.hstack((color1, color2, color3))\n",
    "                        \n",
    "def color_hist(img, nbins=32):    #bins_range=(0, 256)\n",
    "    # Compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins)\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_img_features(img, color_space='BGR', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel='ALL',\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):    \n",
    "    #1) Define an empty list to receive features\n",
    "    img_features = []\n",
    "    #2) Apply color conversion if other than 'RGB'\n",
    "    if color_space != 'BGR':\n",
    "        if color_space == 'HSV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        elif color_space == 'LUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_BGR2LUV)\n",
    "        elif color_space == 'HLS':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_BGR2HLS)\n",
    "        elif color_space == 'YUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "        elif color_space == 'YCrCb':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
    "    else: feature_image = np.copy(img)      \n",
    "    #3) Compute spatial features if flag is set\n",
    "    if spatial_feat == True:\n",
    "        spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "        #4) Append features to list\n",
    "        img_features.append(spatial_features)\n",
    "    #5) Compute histogram features if flag is set\n",
    "    if hist_feat == True:\n",
    "        hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "        #6) Append features to list\n",
    "        img_features.append(hist_features)\n",
    "    #7) Compute HOG features if flag is set\n",
    "    if hog_feat == True:\n",
    "        if hog_channel == 'ALL':\n",
    "            hog_features = []\n",
    "            for channel in range(feature_image.shape[2]):\n",
    "                hog_features.extend(get_hog_features(feature_image[:,:,channel], \n",
    "                                    orient, pix_per_cell, cell_per_block, \n",
    "                                    vis=False, feature_vec=True))      \n",
    "        else:\n",
    "            hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                        pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "        #8) Append features to list\n",
    "        img_features.append(hog_features)\n",
    "\n",
    "    #9) Return concatenated array of features\n",
    "    return np.concatenate(img_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = '../Plot3D_color_explore/images/53.jpg'\n",
    "img = cv2.imread(test_img)\n",
    "\n",
    "test_features = single_img_features(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8460,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "class eSVC(SVC):\n",
    "    def __init__(self, \n",
    "                \n",
    "                color_space='BGR', spatial_size=(32, 32),\n",
    "                hist_bins=32, orient=9, \n",
    "                pix_per_cell=8, cell_per_block=2, hog_channel='ALL',\n",
    "                spatial_feat=True, hist_feat=True, hog_feat=True,\n",
    "                \n",
    "                C=1.0, kernel='rbf', degree=3, gamma='auto', \n",
    "                 coef0=0.0, shrinking=True, probability=False, \n",
    "                 tol=0.001, cache_size=200, class_weight=None, \n",
    "                 verbose=False, max_iter=-1, decision_function_shape='ovr', \n",
    "                 random_state=None):\n",
    "        \n",
    "        self.color_space=color_space\n",
    "        self.spatial_size=spatial_size\n",
    "        self.hist_bins=hist_bins \n",
    "        self.orient=orient\n",
    "        self.pix_per_cell=pix_per_cell\n",
    "        self.cell_per_block=cell_per_block \n",
    "        self.hog_channel=hog_channel\n",
    "        self.spatial_feat=spatial_feat\n",
    "        self.hist_feat=hist_feat\n",
    "        self.hog_feat=hog_feat\n",
    "        \n",
    "        SVC.__init__(self, C=C, kernel=kernel, degree=degree, gamma=gamma, \n",
    "                 coef0=coef0, shrinking=shrinking, probability=probability, \n",
    "                 tol=tol, cache_size=cache_size, class_weight=class_weight, \n",
    "                 verbose=verbose, max_iter=max_iter, decision_function_shape=decision_function_shape, \n",
    "                 random_state=random_state)\n",
    "        \n",
    "    '''\n",
    "    fit() takes an array of images as input and extracts features. The features are then fed into the classifier\n",
    "    \n",
    "    X: array-like, elements are BGR image arrays\n",
    "    y: labels for each image\n",
    "    sample-weights: passed on to SVC().fit()\n",
    "    '''    \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        X_features = []\n",
    "        for x in X:\n",
    "            X_features.append(single_img_features(x, color_space=self.color_space, spatial_size=self.spatial_size,\n",
    "                hist_bins=self.hist_bins, orient=self.orient, \n",
    "                pix_per_cell=self.pix_per_cell, cell_per_block=self.cell_per_block, hog_channel=self.hog_channel,\n",
    "                spatial_feat=self.spatial_feat, hist_feat=self.hist_feat, hog_feat=self.hog_feat))\n",
    "        \n",
    "        return SVC.fit(self, np.array(X_features), y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_features = []\n",
    "        \n",
    "        if len(X.shape) == 3:\n",
    "            X_features.append(single_img_features(X, color_space=self.color_space, spatial_size=self.spatial_size,\n",
    "                hist_bins=self.hist_bins, orient=self.orient, \n",
    "                pix_per_cell=self.pix_per_cell, cell_per_block=self.cell_per_block, hog_channel=self.hog_channel,\n",
    "                spatial_feat=self.spatial_feat, hist_feat=self.hist_feat, hog_feat=self.hog_feat))\n",
    "        else:\n",
    "            for x in X:\n",
    "                X_features.append(single_img_features(x, color_space=self.color_space, spatial_size=self.spatial_size,\n",
    "                    hist_bins=self.hist_bins, orient=self.orient, \n",
    "                    pix_per_cell=self.pix_per_cell, cell_per_block=self.cell_per_block, hog_channel=self.hog_channel,\n",
    "                    spatial_feat=self.spatial_feat, hist_feat=self.hist_feat, hog_feat=self.hog_feat))\n",
    "                \n",
    "        return SVC.predict(self, np.array(X_features))\n",
    "        \n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        \n",
    "        return SVC.score(self, X, y)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Search ###\n",
    "Using the above subclass and GridSearchCV from sklearn, we look for the best parametres for feature extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create image arrays for use in GridSearchCV\n",
    "X_sample_img = []\n",
    "test_sample_size = 700\n",
    "for i in range(test_sample_size):\n",
    "    X_sample_img.append(cv2.imread(cars[i]))\n",
    "for i in range(test_sample_size):\n",
    "    X_sample_img.append(cv2.imread(notcars[i]))\n",
    "X_sample_img = np.array(X_sample_img)\n",
    "y = np.concatenate((np.ones(test_sample_size), np.zeros(test_sample_size))).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_sample_img, y = shuffle(X_sample_img, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param = {\n",
    "    'color_space':['HLS', 'HSV', 'LUV'], \n",
    "    'spatial_size':[(32, 32)],\n",
    "    'hist_bins':(32,16), \n",
    "    'orient':(9,12), \n",
    "    'pix_per_cell':(6,8), \n",
    "    'cell_per_block':(2,3), \n",
    "    'hog_channel':['ALL'],\n",
    "                \n",
    "    'C':(1.0, 1000.0), \n",
    "    'kernel':['linear'], \n",
    "    'gamma':(0.01, 100), \n",
    "    \n",
    "}\n",
    "clf_eSVC = eSVC()\n",
    "grid_search = GridSearchCV(clf_eSVC, param_grid=param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=eSVC(C=1.0, cache_size=200, cell_per_block=2, class_weight=None, coef0=0.0,\n",
       "   color_space='BGR', decision_function_shape='ovr', degree=3,\n",
       "   gamma='auto', hist_bins=32, hist_feat=True, hog_channel='ALL',\n",
       "   hog_feat=True, kernel='rbf', max_iter=-1, orient=9, pix_per_cell=8,\n",
       "   probability=False, random_state=None, shrinking=True, spatial_feat=True,\n",
       "   spatial_size=(32, 32), tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'kernel': ['linear'], 'hist_bins': (32, 16), 'pix_per_cell': (6, 8), 'cell_per_block': (2, 3), 'color_space': ['HLS', 'HSV', 'LUV'], 'gamma': (0.01, 100), 'orient': (9, 12), 'hog_channel': ['ALL'], 'C': (1.0, 1000.0), 'spatial_size': [(32, 32)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_sample_img, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridscv = {'gscv': grid_search}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(gridscv, open('gridObject', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'cell_per_block': 2,\n",
       " 'color_space': 'HLS',\n",
       " 'gamma': 0.01,\n",
       " 'hist_bins': 16,\n",
       " 'hog_channel': 'ALL',\n",
       " 'kernel': 'linear',\n",
       " 'orient': 9,\n",
       " 'pix_per_cell': 6,\n",
       " 'spatial_size': (32, 32)}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99357142857142855"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'color_space':['HLS', 'YUV'], \n",
    "    'spatial_size':[(32, 32), (64,64)],\n",
    "    'hist_bins':[16], \n",
    "    'orient':[9], \n",
    "    'pix_per_cell':[6], \n",
    "    'cell_per_block':[2], \n",
    "    'hog_channel':['ALL'],\n",
    "                \n",
    "    'C':(1.0, 1000.0), \n",
    "    'kernel':['linear', 'rbf', 'sigmoid'], \n",
    "    'gamma':(0.01, 100), \n",
    "    \n",
    "}\n",
    "clf_eSVC_2 = eSVC()\n",
    "grid_search_2 = GridSearchCV(clf_eSVC, param_grid=param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=eSVC(C=1.0, cache_size=200, cell_per_block=2, class_weight=None, coef0=0.0,\n",
       "   color_space='BGR', decision_function_shape='ovr', degree=3,\n",
       "   gamma='auto', hist_bins=32, hist_feat=True, hog_channel='ALL',\n",
       "   hog_feat=True, kernel='rbf', max_iter=-1, orient=9, pix_per_cell=8,\n",
       "   probability=False, random_state=None, shrinking=True, spatial_feat=True,\n",
       "   spatial_size=(32, 32), tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'kernel': ['linear', 'rbf', 'sigmoid'], 'hist_bins': [16], 'pix_per_cell': [6], 'cell_per_block': [2], 'color_space': ['HLS', 'YUV'], 'gamma': (0.01, 100), 'orient': [9], 'hog_channel': ['ALL'], 'C': (1.0, 1000.0), 'spatial_size': [(32, 32), (64, 64)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_2.fit(X_sample_img, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'cell_per_block': 2,\n",
       " 'color_space': 'HLS',\n",
       " 'gamma': 0.01,\n",
       " 'hist_bins': 16,\n",
       " 'hog_channel': 'ALL',\n",
       " 'kernel': 'linear',\n",
       " 'orient': 9,\n",
       " 'pix_per_cell': 6,\n",
       " 'spatial_size': (32, 32)}"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99357142857142855"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_2.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** NOTE: ** Several iterations of grid search were run in the same cells above, by varying the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best parametres for *feature extraction* are: **\n",
    "\n",
    "- cell_per_block: 2,\n",
    "- color_space: 'HLS',\n",
    "- hist_bins: 16,\n",
    "- hog_channel: 'ALL',\n",
    "- orient: 9,\n",
    "- pix_per_cell: 6,\n",
    "- spatial_size': (32, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier and Parametre Search ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also from above, the best parametres for the **SVM classifier**, reporting a **best score of 0.99357** are:\n",
    "\n",
    "- kernel: 'linear'\n",
    "- C: 1.0\n",
    "\n",
    "We find below that a **Random Forest Classifier** does a slightly better job albeit, **yeilding 0.995 score** with the following best parametres:\n",
    "\n",
    "- criterion: 'entropy',\n",
    "- min_samples_leaf: 1,\n",
    "- min_samples_split: 2,\n",
    "- n_estimators: 7\n",
    "\n",
    "We'll have to explore both classifiers with the full dataset in the next section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_estimators':[7, 8, 10], \n",
    "    'criterion':['entropy'],  \n",
    "    'min_samples_split':[2], \n",
    "    'min_samples_leaf':[1]\n",
    "    \n",
    "}\n",
    "clf_rf = RandomForestClassifier()\n",
    "rf_grid = GridSearchCV(clf_rf, rf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = []\n",
    "for x in X_sample_img:\n",
    "    X_sample.append(single_img_features(x, color_space='HLS', spatial_size=(32,32),\n",
    "    hist_bins=16, orient=9, \n",
    "    pix_per_cell=6, cell_per_block=2, hog_channel='ALL',\n",
    "    spatial_feat=True, hist_feat=True, hog_feat=True))\n",
    "X_sample = np.array(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.29 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'min_samples_split': [2], 'n_estimators': [7, 8, 10], 'min_samples_leaf': [1], 'criterion': ['entropy']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time rf_grid.fit(X_sample, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 8}"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98928571428571432"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: 0.98429, std: 0.02067, params: {'min_samples_leaf': 1, 'min_samples_split': 2, 'criterion': 'gini', 'n_estimators': 7},\n",
       " mean: 0.98714, std: 0.01362, params: {'min_samples_leaf': 1, 'min_samples_split': 2, 'criterion': 'gini', 'n_estimators': 15},\n",
       " mean: 0.98929, std: 0.01061, params: {'min_samples_leaf': 1, 'min_samples_split': 2, 'criterion': 'gini', 'n_estimators': 20},\n",
       " mean: 0.97857, std: 0.02143, params: {'min_samples_leaf': 1, 'min_samples_split': 10, 'criterion': 'gini', 'n_estimators': 7},\n",
       " mean: 0.98429, std: 0.01921, params: {'min_samples_leaf': 1, 'min_samples_split': 10, 'criterion': 'gini', 'n_estimators': 15},\n",
       " mean: 0.98643, std: 0.01611, params: {'min_samples_leaf': 1, 'min_samples_split': 10, 'criterion': 'gini', 'n_estimators': 20},\n",
       " mean: 0.97643, std: 0.02280, params: {'min_samples_leaf': 20, 'min_samples_split': 2, 'criterion': 'gini', 'n_estimators': 7},\n",
       " mean: 0.98071, std: 0.01429, params: {'min_samples_leaf': 20, 'min_samples_split': 2, 'criterion': 'gini', 'n_estimators': 15},\n",
       " mean: 0.98071, std: 0.01681, params: {'min_samples_leaf': 20, 'min_samples_split': 2, 'criterion': 'gini', 'n_estimators': 20},\n",
       " mean: 0.97429, std: 0.02346, params: {'min_samples_leaf': 20, 'min_samples_split': 10, 'criterion': 'gini', 'n_estimators': 7},\n",
       " mean: 0.97857, std: 0.02011, params: {'min_samples_leaf': 20, 'min_samples_split': 10, 'criterion': 'gini', 'n_estimators': 15},\n",
       " mean: 0.98214, std: 0.01815, params: {'min_samples_leaf': 20, 'min_samples_split': 10, 'criterion': 'gini', 'n_estimators': 20},\n",
       " mean: 0.98643, std: 0.01320, params: {'min_samples_leaf': 1, 'min_samples_split': 2, 'criterion': 'entropy', 'n_estimators': 7},\n",
       " mean: 0.99214, std: 0.00660, params: {'min_samples_leaf': 1, 'min_samples_split': 2, 'criterion': 'entropy', 'n_estimators': 15},\n",
       " mean: 0.99071, std: 0.00860, params: {'min_samples_leaf': 1, 'min_samples_split': 2, 'criterion': 'entropy', 'n_estimators': 20},\n",
       " mean: 0.98786, std: 0.01261, params: {'min_samples_leaf': 1, 'min_samples_split': 10, 'criterion': 'entropy', 'n_estimators': 7},\n",
       " mean: 0.99143, std: 0.00906, params: {'min_samples_leaf': 1, 'min_samples_split': 10, 'criterion': 'entropy', 'n_estimators': 15},\n",
       " mean: 0.99143, std: 0.00923, params: {'min_samples_leaf': 1, 'min_samples_split': 10, 'criterion': 'entropy', 'n_estimators': 20},\n",
       " mean: 0.98000, std: 0.01505, params: {'min_samples_leaf': 20, 'min_samples_split': 2, 'criterion': 'entropy', 'n_estimators': 7},\n",
       " mean: 0.98643, std: 0.01021, params: {'min_samples_leaf': 20, 'min_samples_split': 2, 'criterion': 'entropy', 'n_estimators': 15},\n",
       " mean: 0.98643, std: 0.01021, params: {'min_samples_leaf': 20, 'min_samples_split': 2, 'criterion': 'entropy', 'n_estimators': 20},\n",
       " mean: 0.99071, std: 0.00612, params: {'min_samples_leaf': 20, 'min_samples_split': 10, 'criterion': 'entropy', 'n_estimators': 7},\n",
       " mean: 0.98357, std: 0.01160, params: {'min_samples_leaf': 20, 'min_samples_split': 10, 'criterion': 'entropy', 'n_estimators': 15},\n",
       " mean: 0.98786, std: 0.00960, params: {'min_samples_leaf': 20, 'min_samples_split': 10, 'criterion': 'entropy', 'n_estimators': 20}]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid.grid_scores_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on all samples \n",
    "   (save test set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "### extract features for all images in the training dataset (cars, notcars) using the optimal parametres found above\n",
    "for img_name in cars:\n",
    "    img = cv2.imread(img_name)\n",
    "    img_features = single_img_features(img, color_space='HLS', spatial_size=(32, 32),\n",
    "                        hist_bins=16, orient=9, \n",
    "                        pix_per_cell=6, cell_per_block=2, hog_channel='ALL',\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True)\n",
    "    X.append(img_features)\n",
    "for img_name in notcars:\n",
    "    img = cv2.imread(img_name)\n",
    "    img_features = single_img_features(img, color_space='HLS', spatial_size=(32, 32),\n",
    "                        hist_bins=16, orient=9, \n",
    "                        pix_per_cell=6, cell_per_block=2, hog_channel='ALL',\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True)\n",
    "    X.append(img_features)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.concatenate((np.ones(len(cars)), np.zeros(len(notcars))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Scale the features using the training set for transform fit: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaler = StandardScaler()\n",
    "X_scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and test Random Forest Classifier: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_randomForest = RandomForestClassifier(criterion='entropy', n_estimators=30, min_samples_leaf=1, min_samples_split=2)\n",
    "\n",
    "%time clf_randomForest.fit(X_train_scaled_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 62.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.96621621621621623"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf_randomForest.score(X_test_scaled_pca, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and test SVM classifier: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svc = SVC(kernel='linear', C=1.0)\n",
    "%time clf_svc.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.99296171171171166"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf_svc.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Trying NB classifier** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.65 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf_nb = GaussianNB()\n",
    "%time clf_nb.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.29 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94538288288288286"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf_nb.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying **AdaBoostClassifier**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf_ada = AdaBoostClassifier()\n",
    "%time clf_ada.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.77 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98001126126126126"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf_ada.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Save the classifiers and the scaler: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_scaler = {'clf_svc': clf_svc, 'X_scaler': X_scaler}\n",
    "pickle.dump(svc_scaler, open('svc_scaler.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_scaler = {'clf_rf': clf_randomForest, 'X_scaler': X_scaler}\n",
    "pickle.dump(rf_scaler, open('rf_scaler.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_scaler = {'clf_ada': clf_ada, 'X_scaler': X_scaler}\n",
    "pickle.dump(ada_scaler, open('ada_scaler.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add PCA dimensionality reduction ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above SVM classifier was to slow in performing classification and thus we look into dimensionality reduction of feature set using Principal Component Ananlysis below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=600, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=600)\n",
    "%time pca.fit(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.79 s\n"
     ]
    }
   ],
   "source": [
    "%time X_train_scaled_pca = pca.transform(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.12 s\n"
     ]
    }
   ],
   "source": [
    "%time X_test_scaled_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svc_pca = SVC(kernel='linear')\n",
    "%time clf_svc_pca.fit(X_train_scaled_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.15 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98536036036036034"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf_svc_pca.score(X_test_scaled_pca, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Save the PCA object along with corresponding SVC and scaler:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_dict = {'clf_svc_pca': clf_svc_pca, 'pca': pca, 'scaler': X_scaler }\n",
    "pickle.dump(svc_dict, open('svc_pca_scaler.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
